{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3936181d",
   "metadata": {},
   "source": [
    "# 利用Evo2模型作为基模，添加全连接层与分类头，用于训练增强子活性预测模型\n",
    "\n",
    "以下为代码\n",
    "\n",
    "添加L2/L1正则化，防止过拟合\n",
    "\n",
    "train loss 大于val loss，是因为：\n",
    "1. Dropout 层的不同行为 (最主要原因)\n",
    "你的模型 Evo2ForRegression 中使用了一个 nn.Dropout 层。Dropout 是一种正则化技术，用于防止模型过拟合。它的工作原理如下：\n",
    "在训练阶段 (model.train())：train_epoch 函数中调用了 model.train()。在这种模式下，Dropout 层会随机地将一部分神经元的输出设置为 0（在你的配置中，dropout_rate = 0.1，即 10% 的神经元会被“丢弃”）。这相当于每次训练时都在使用一个“削弱”了的网络，强迫网络学习更加鲁棒的特征，但这也会导致训练时的损失值偏高。\n",
    "在评估阶段 (model.eval())：evaluate 函数中调用了 model.eval()。在这种模式下，Dropout 层会被禁用，所有的神经元都会被用于计算。模型会使用其完整的学习能力进行预测。\n",
    "因此，你在验证时使用的是一个“完整”且“更强大”的模型，而在训练时使用的是一个被随机“削弱”的模型。这就导致了在相同数据上，验证损失会显著低于训练损失。\n",
    "2. 训练过程与评估过程的差异\n",
    "除了 Dropout，训练和评估在计算损失的方式上也有一个微妙但重要的区别：\n",
    "训练损失：是你看到的一整个 epoch 期间所有批次（batch）损失的平均值。在一个 epoch 中，模型的权重在每个批次结束后都会通过反向传播进行更新。这意味着，epoch 开始时的模型比结束时的模型要“差”。训练损失包含了模型在“学习进步”过程中的所有表现，包括了最开始那些比较高的损失值。\n",
    "验证损失：是在整个训练 epoch 结束之后，使用该 epoch 最终更新好的、固定的模型权重，对验证集进行一次完整评估计算出的损失。它反映的是模型在当前 epoch 训练结束后的“最终实力”。\n",
    "综合来看，验证损失是“学成之后”的摸底考试成绩，而训练损失是“一边学习一边考试”的平时成绩平均分，所以前者通常会更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fda4e7",
   "metadata": {},
   "source": [
    "## 导入库\n",
    "\n",
    "导入了训练增强子活性预测模型所需的主要库，包括PyTorch、pandas、tqdm等。  \n",
    "尝试导入Evo2模型作为基模。如果未找到Evo2库，会提示错误并退出。这样为后续的数据处理、模型构建和训练做好了准备。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1df5fb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader, random_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\__init__.py:274\u001b[0m\n\u001b[0;32m    270\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    272\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 274\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\__init__.py:250\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    248\u001b[0m is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 250\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mkernel32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     last_error \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mget_last_error()\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Import Evo2\n",
    "try:\n",
    "    from evo2 import Evo2\n",
    "except ImportError:\n",
    "    print(\"未找到Evo2库，请确保已正确安装。\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf654c57",
   "metadata": {},
   "source": [
    "## 定义类：数据集Evo2RegressionDataset\n",
    "\n",
    "创建了一个名为`Evo2RegressionDataset`的自定义Dataset类，用于处理增强子活性预测的数据。该类实现以下功能：\n",
    "\n",
    "1. **初始化**：加载包含DNA序列和表达活性的CSV文件，并设置Evo2分词器\n",
    "2. **数据验证**：确保CSV文件包含'sequence'和'expression'必要列\n",
    "3. **数据获取**：将DNA序列转换为模型可用的token序列，对序列进行填充或截断，确保所有输入长度一致，最后结合expression数据项根据索引 idx 返回模型训练所需的单条样本，格式为{ 'input_ids': sequence数据项分词后的序列, 'target': expression数据项对应的浮点张量 }\n",
    "\n",
    "这个类为后续模型训练提供了标准化的数据接口，使DNA序列能够被Evo2模型处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evo2RegressionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    为回归模型加载包含增强子DNA序列和活性的数据集。\n",
    "    使用Evo2模型提供的分词器。\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            csv_file (str): 包含'sequence'和'expression'列的csv文件路径。\n",
    "            tokenizer (object): 来自Evo2的分词器实例。\n",
    "            max_length (int): 用于填充/截断的最大序列长度。\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data = pd.read_csv(csv_file)\n",
    "            # 确保CSV文件包含必要的列\n",
    "            if 'sequence' not in self.data.columns or 'expression' not in self.data.columns:\n",
    "                raise ValueError(\"CSV文件必须包含'sequence'和'expression'列。\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"错误：在{csv_file}路径未找到CSV文件\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"读取CSV文件时出错：{e}\")\n",
    "            raise\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # 根据索引 idx 返回模型训练所需的单条样本，格式为{ 'input_ids': sequence数据项分词后的序列, 'target': expression数据项对应的浮点张量 }\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data):\n",
    "            raise IndexError(\"索引超出范围\")\n",
    "\n",
    "        # 获取指定索引idx的对应的sequence数据项，存储在seq中\n",
    "        seq = self.data.iloc[idx]['sequence'] \n",
    "        # 确保sequence列元素是字符串类型\n",
    "        if not isinstance(seq, str):\n",
    "            seq = str(seq)\n",
    "\n",
    "        # 获取对应的expression数据项，存储在target中\n",
    "        target = float(self.data.iloc[idx]['expression'])\n",
    "\n",
    "        # 使用Evo2的分词器对序列进行分词\n",
    "        # 注意：Evo2使用charleveltokenizer，不需要像HuggingFace分词器那样的显式填充/截断参数\n",
    "        # 它通常基于字符到整数的映射进行分词。我们需要手动进行填充/截断。\n",
    "        token_ids = self.tokenizer.tokenize(seq)\n",
    "\n",
    "        # 手动进行填充/截断\n",
    "        if len(token_ids) > self.max_length:\n",
    "            token_ids = token_ids[:self.max_length]\n",
    "        else:\n",
    "            padding_length = self.max_length - len(token_ids)\n",
    "            #charleveltokenizer 的 pad token id = 1\n",
    "            pad_token_id = getattr(self.tokenizer, 'pad_token_id', 1)\n",
    "            token_ids.extend([pad_token_id] * padding_length)\n",
    "\n",
    "        input_ids = torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'target': torch.tensor(target, dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cffbffe",
   "metadata": {},
   "source": [
    "## 定义类：基于evo2+全连接+回归头的活性预测模型Evo2RegressionModel\n",
    "\n",
    "创建了一个名为`Evo2ForRegression`的模型类，继承自PyTorch的nn.Module，用于增强子活性预测任务。\n",
    "\n",
    "1. **基础架构**：利用预训练的Evo2模型作为特征提取器，并冻结其参数以保留已学习的DNA表示能力\n",
    "2. **组件构成**：\n",
    "    - 加载并冻结Evo2基础模型\n",
    "    - 添加全连接层(FC)将Evo2输出映射到中间表示\n",
    "    - 使用ReLU激活函数和Dropout层防止过拟合\n",
    "    - 最后通过回归头预测增强子活性值\n",
    "\n",
    "3. **前向传播流程**：\n",
    "    - 从Evo2的第28层提取DNA序列嵌入\n",
    "    - 对序列维度进行平均池化，获取整个序列的全局表示\n",
    "    - 通过全连接层、激活函数和Dropout层处理\n",
    "    - 最终输出单一的回归预测值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c927825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evo2ForRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    基于evo2+全连接+回归头的活性预测模型。\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"evo2_7b\", local_path='/root/autodl-tmp/evo2/models/evo2-7b/evo2_7b.pt', dropout_rate=0.1, intermediate_size=512):\n",
    "        super().__init__()\n",
    "        print(f\"加载 Evo2 模型: {model_name}...\")\n",
    "        try:\n",
    "            self.evo2_wrapper = Evo2(model_name=model_name, local_path=local_path)\n",
    "        except Exception as e:\n",
    "            print(f\"加载 Evo2 模型 '{model_name}' 时出错: {e}\")\n",
    "            print(\"请确认模型名称是否正确，以及相关依赖是否已安装。\")\n",
    "            raise\n",
    "\n",
    "        # 冻结Evo2基础模型参数\n",
    "        print(\"正在冻结Evo2基础模型参数...\")\n",
    "        for param in self.evo2_wrapper.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # 从模型配置中获取隐藏层大小\n",
    "        hidden_size = self.evo2_wrapper.model.config.hidden_size\n",
    "\n",
    "        # 新的全连接层\n",
    "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # 回归头：从intermediate_size到1的线性层（预测活性/expression这一标量值）\n",
    "        self.regressor = nn.Linear(intermediate_size, 1)\n",
    "\n",
    "        # 将新层转换为bfloat16以匹配预期的输入数据类型\n",
    "        self.fc1.to(torch.bfloat16)\n",
    "        self.regressor.to(torch.bfloat16)\n",
    "        print(f\"Evo2ForRegression初始化完成，隐藏层大小={hidden_size}, 全连接与回归头中间层大小={intermediate_size}。\")\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        前向传播（通过Evo2基础模型，全连接层和回归头）。\n",
    "\n",
    "        参数：\n",
    "            input_ids (torch.Tensor): 标记ID张量 (batch_size, sequence_length)。\n",
    "\n",
    "        返回：\n",
    "            torch.Tensor: 回归预测值 (batch_size,1)。如batch_size=8,返回形如[34.5, 25.125, 26.125, 73.0, 0.88671875, 44.75, 28.75, -27.125]是一次处理8个样本的预测结果。\n",
    "        \"\"\"\n",
    "       \n",
    "        # 从指定的层提取DNA增强子序列的embeddings。\n",
    "        layer_to_embed = \"blocks.28.mlp.l3\"\n",
    "\n",
    "         # 让输入参数通过Evo2基础模型\n",
    "        _, embeddings = self.evo2_wrapper(input_ids, return_embeddings=True, layer_names=[layer_to_embed]) #self.evo2_wrapper是一个evo2实例，它接受左边的参数进行前向传播，返回两个参数：第一个是模型的输出，第二个是指定层（这里为layer_to_embed层）的嵌入（embeddings）。\n",
    "        last_layer_output = embeddings[layer_to_embed] #last_layer_output是一个三维pytorch张量，形状为[batch_size, sequence_length, hidden_size]。\n",
    "        #本程序中batch_size=和sequence_length在main函数指定, hidden_size=4096。所以last_layer_output是一个1 x sequence_length x 4096的三维数组/矩阵，数组的[0][a-1][i-1]元素代表输入序列中第i个碱基对应的4096维嵌入向量中第i维的嵌入数值。\n",
    "\n",
    "\n",
    "        # 在sequence_length维度上池化隐藏状态\n",
    "        # 使用平均池化：在序列长度上平均嵌入（平均池化的意思：一共sequence_length个位置，每个位置都有一个4096维嵌入向量，将所以这些位置的sequence_length个4096维向量做平均，得到最终的一个4096维向量，这个向量包含了整个序列的全局信息）\n",
    "        # 形状：(batch_size, hidden_size=4096)\n",
    "        pooled_output = last_layer_output.mean(dim=1)\n",
    "\n",
    "        # 通过新的全连接层和激活函数\n",
    "        x = self.fc1(pooled_output)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # 应用dropout和回归头\n",
    "        x = self.dropout(x)\n",
    "        prediction = self.regressor(x)\n",
    "\n",
    "        # x形如[[34.5], [25.125], [26.125], [73.0], [0.88671875], [44.75], [28.75], [-27.125]]（batch_size=8），是一次处理8个样本的预测结果。\n",
    "\n",
    "        # 压缩输出，去掉多余维度以获得形状(batch_size,1)\n",
    "        #prediction形如[34.5, 25.125, 26.125, 73.0, 0.88671875, 44.75, 28.75, -27.125]是一次处理8个样本的预测结果。\n",
    "        \n",
    "        return prediction.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075de86a",
   "metadata": {},
   "source": [
    "## 定义训练与验证函数\n",
    "\n",
    "1. **`train_epoch` 函数**：  \n",
    "    - 用于训练模型一个周期。\n",
    "    - 将模型设置为训练模式 (`model.train()`)，并通过 `tqdm` 显示训练进度。\n",
    "    - 对每个批次数据进行前向传播、计算损失、反向传播和优化。\n",
    "    - 累加所有批次的损失并除以数据集数据个数以计算整个数据集的平均损失。\n",
    "\n",
    "2. **`evaluate` 函数**：  \n",
    "    - 用于在验证集上评估模型性能。\n",
    "    - 将模型设置为评估模式 (`model.eval()`)，并禁用梯度计算 (`torch.no_grad()`)，以提高评估效率。\n",
    "    - 对每个批次数据进行前向传播和损失计算。\n",
    "    - 累加所有批次的损失并除以验证集数据个数以计算整个验证集的平均损失。\n",
    "\n",
    "这两个函数为模型的训练和验证过程提供了标准化的接口，确保训练过程的可控性和验证过程的准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3448733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    \"\"\"用于训练模型一个周期的函数\"\"\"\n",
    "    model.train()  # 将模型设置为训练模式\n",
    "    total_loss = 0\n",
    "    # 用tqdm包装data_loader以显示进度条（可选）\n",
    "    progress_bar = tqdm(data_loader, desc=\"训练中\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        #input_ids: 是一个张量 (Tensor)，代表了批处理 (batch) 中所有 DNA 序列经过分词和填充/截断后的结果。它的形状是 (batch_size, max_length)。在 main 函数配置中，batch_size 默认为 8，max_length 默认为 512。因此，input_ids 的具体形状是 (8, 512)。\n",
    "\n",
    "        targets = batch['target'].to(device, dtype=torch.bfloat16)\n",
    "        #targets: 是一个张量，代表了批处理中每个 DNA 序列对应的增强子活性值 (expression)。它的形状是 (batch_size)。在配置中，batch_size 为 8，所以 targets 的具体形状是 (8)。\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "        #loss 通常是一个 PyTorch 张量（Tensor），它代表了模型在当前这一个批次（batch）数据上的平均损失。例如，如果你的批次大小是 32，那么 loss 就代表了这 32 个样本的平均损失值。它是一个只包含一个元素的零维张量，例如 tensor(0.1234)。\n",
    "        #接下来是 .item() 方法。这是一个非常重要的方法。直接对 PyTorch 张量进行累加会保留其计算图（computation graph），这会在每个循环中不断消耗内存，最终可能导致内存溢出。.item() 方法的作用是从一个只包含单个值的张量中提取出其对应的标准 Python 数字（通常是浮点数），并且不保留任何计算图信息。这样，我们就可以安全地用它来进行累加计算。\n",
    "        #input_ids.size(0) 用来获取当前批次的大小。数据通常以 (batch_size, ...) 的形式组织。因此，.size(0) 或 .shape[0] 会返回第一个维度的大小，也就是批次中的样本数量。\n",
    "        #loss.item() * input_ids.size(0)。这个操作的目的是将平均损失转换回这个批次的总损失。因为 loss 是批次的平均损失，所以将它乘以批次中的样本数，就得到了这个批次所有样本的损失之和。这对于精确计算整个数据集的总损失至关重要，特别是当最后一个批次的大小可能小于常规批次大小时。\n",
    "        #total_loss += ... 这部分就是将当前批次计算出的总损失累加到 total_loss 变量中。在遍历完所有数据批次后，total_loss 变量将包含整个数据集的总损失。\n",
    "\n",
    "\n",
    "        # 更新进度条描述（可选）\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    \"\"\"在验证集上评估模型。\"\"\"\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # 禁用梯度计算\n",
    "        # 用tqdm包装data_loader以显示进度条（可选）\n",
    "        progress_bar = tqdm(data_loader, desc=\"评估中\", leave=False)\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            targets = batch['target'].to(device, dtype=torch.bfloat16)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(input_ids)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            # 更新进度条描述（可选）\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32874de6",
   "metadata": {},
   "source": [
    "## 主程序，配置参数，处理训练和评估过程\n",
    "\n",
    "主程序定义了完整的训练流程，包括参数配置、数据加载、模型初始化和训练循环：\n",
    "\n",
    "**参数配置**：\n",
    "- 设置模型名称、数据文件路径、输出目录等基本参数\n",
    "- 配置训练超参数：批大小(8)、训练轮数(100)、学习率(1e-5)等\n",
    "- 设置模型架构参数：序列长度(512)、dropout率(0.1)、隐藏层大小(512)\n",
    "\n",
    "**环境准备**：\n",
    "- 自动检测并设置GPU/CPU设备\n",
    "- 创建输出目录用于保存训练结果\n",
    "\n",
    "**模型与数据初始化**：\n",
    "- 加载Evo2回归模型，包含预训练基础模型和自定义回归头\n",
    "- 创建数据集实例，按8:2比例分割训练集和验证集\n",
    "- 构建DataLoader用于批处理训练数据\n",
    "\n",
    "**训练配置**：\n",
    "- 使用AdamW优化器，仅优化回归头参数（冻结Evo2基础模型）\n",
    "- 采用MSE损失函数进行回归任务\n",
    "- 实现早停机制：监控验证损失，自动保存性能最佳的模型\n",
    "\n",
    "**继续训练功能**：\n",
    "- 支持从已保存的模型检查点继续训练\n",
    "- 程序启动时询问用户是否继续训练已有模型\n",
    "- 如果选择继续训练，会加载已保存的模型权重并评估初始验证损失\n",
    "- 自动处理检查点文件不存在的情况，提供友好的错误提示\n",
    "\n",
    "**训练循环**：\n",
    "- 每轮训练包含完整的训练和验证过程\n",
    "- 实时显示训练进度和损失变化\n",
    "- 当验证损失改善时自动保存模型权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466c703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- 配置参数 ---\n",
    "    model_name = \"evo2_7b\" # Evo2模型名称\n",
    "    csv_file = \"data.csv\" # 包含DNA序列和活性数据的CSV文件路径\n",
    "    output_dir = \"evo2_regression_output\" # 保存模型的目录\n",
    "    output_model_file = os.path.join(output_dir, f\"{model_name}_regression.pt\")\n",
    "\n",
    "    max_length = 512   # tokenizer的最大序列长度\n",
    "    batch_size = 8     # 批处理大小，根据显存情况调整\n",
    "    epochs = 100       # 训练轮数\n",
    "    learning_rate = 1e-5 # 回归头的学习率\n",
    "    train_split = 0.8  # 训练数据占数据集的比例\n",
    "    dropout_rate = 0.1 # 回归头的dropout率\n",
    "    intermediate_hidden_size = 512 # 全连接隐藏层大小\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)    # 如果输出目录不存在则创建\n",
    "\n",
    "    # --- 设备设置 ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"使用设备: {device}\")\n",
    "    if device == torch.device(\"cpu\"):\n",
    "        print(\"警告：正在使用CPU。训练可能会很慢。\")\n",
    "\n",
    "    # --- 加载模型和分词器 ---\n",
    "    try:\n",
    "        model = Evo2ForRegression(model_name=model_name, dropout_rate=dropout_rate, intermediate_size=intermediate_hidden_size)\n",
    "        tokenizer = model.evo2_wrapper.tokenizer \n",
    "    except Exception as e:\n",
    "        print(f\"初始化模型失败: {e}\")\n",
    "        return # 如果模型加载失败则退出\n",
    "    \n",
    "    # 询问是否继续训练\n",
    "    print(\"是否继续训练已保存的模型？(y/n)\")\n",
    "    continue_training = input().strip().lower() == 'y'\n",
    "\n",
    "    # 如果设置了继续训练，则加载已保存的模型权重\n",
    "    if continue_training and os.path.exists(output_model_file):\n",
    "        try:\n",
    "            print(f\"从检查点加载模型: {output_model_file}\")\n",
    "            # 加载状态字典，并确保它被映射到正确的设备\n",
    "            model.load_state_dict(torch.load(output_model_file, map_location=device))\n",
    "            print(\"模型权重加载成功，将继续训练。\")\n",
    "        except Exception as e:\n",
    "            print(f\"加载模型检查点失败: {e}。将从头开始训练。\")\n",
    "    elif continue_training:\n",
    "        print(f\"警告: 未找到模型检查点 {output_model_file}。将从头开始训练。\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    '''\n",
    "    # --- 构建数据集 ---\n",
    "    try:\n",
    "        dataset = Evo2RegressionDataset(csv_file, tokenizer, max_length)\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"加载数据集失败: {e}\")\n",
    "        return\n",
    "\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    '''\n",
    "    \n",
    "    # 构建数据集方法2：直接加载训练集和测试集\n",
    "    try:\n",
    "        train_dataset = Evo2RegressionDataset(\"train_data.csv\", tokenizer, max_length)\n",
    "        val_dataset = Evo2RegressionDataset(\"test_data.csv\", tokenizer, max_length)\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"加载训练集或测试集失败: {e}\")\n",
    "        return\n",
    "\n",
    "    '''\n",
    "    # 临时测试：使用完整数据集作为训练集和验证集\n",
    "    train_dataset = dataset\n",
    "    val_dataset = dataset\n",
    "    '''\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    print(f\"数据集已加载: {len(dataset)} 个样本\")\n",
    "    print(f\"训练集大小: {len(train_dataset)}\")\n",
    "    print(f\"验证集大小: {len(val_dataset)}\")\n",
    "\n",
    "    # --- 设置优化器和损失函数 ---\n",
    "    optimizer = AdamW(list(model.fc1.parameters()) + list(model.regressor.parameters()), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # --- 开始训练 ---\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # 如果继续训练，可以先评估一次当前模型的性能\n",
    "    if continue_training and os.path.exists(output_model_file):\n",
    "        initial_val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"加载模型的初始验证损失: {initial_val_loss:.4f}\")\n",
    "        best_val_loss = initial_val_loss\n",
    "\n",
    "    print(\"开始训练...\")\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"轮次 {epoch+1}/{epochs}: 训练损失 = {train_loss:.4f}, 验证损失 = {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), output_model_file)\n",
    "            print(f\"验证损失已改善。模型已保存至 {output_model_file}\")\n",
    "\n",
    "    print(\"训练完成。\")\n",
    "    print(f\"最佳验证损失: {best_val_loss:.4f}\")\n",
    "    print(f\"最佳模型已保存至: {output_model_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868eb61e",
   "metadata": {},
   "source": [
    "不含继续训练功能的原始main函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b09914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- 配置参数 ---\n",
    "    model_name = \"evo2_7b\" # Evo2模型名称\n",
    "    csv_file = \"data.csv\" # 包含DNA序列和活性数据的CSV文件路径\n",
    "    output_dir = \"evo2_regression_output\" # 保存模型的目录\n",
    "    output_model_file = os.path.join(output_dir, f\"{model_name}_regression.pt\")\n",
    "\n",
    "    max_length = 512   # tokenizer的最大序列长度\n",
    "    batch_size = 8     # 批处理大小，根据显存情况调整\n",
    "    epochs = 100       # 训练轮数\n",
    "    learning_rate = 1e-5 # 回归头的学习率\n",
    "    train_split = 0.8  # 训练数据占数据集的比例\n",
    "    dropout_rate = 0.1 # 回归头的dropout率\n",
    "    intermediate_hidden_size = 512 # 全连接隐藏层大小\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)    # 如果输出目录不存在则创建\n",
    "\n",
    "    # --- 设备设置 ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"使用设备: {device}\")\n",
    "    if device == torch.device(\"cpu\"):\n",
    "        print(\"警告：正在使用CPU。训练可能会很慢。\")\n",
    "\n",
    "    # --- 加载模型和分词器 ---\n",
    "    # Evo2ForRegression类负责加载基础模型和分词器\n",
    "    try:\n",
    "        model = Evo2ForRegression(model_name=model_name, dropout_rate=dropout_rate, intermediate_size=intermediate_hidden_size)\n",
    "        tokenizer = model.evo2_wrapper.tokenizer \n",
    "    except Exception as e:\n",
    "        print(f\"初始化模型失败: {e}\")\n",
    "        return # 如果模型加载失败则退出\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # --- 构建数据集 ---\n",
    "    try:\n",
    "        dataset = Evo2RegressionDataset(csv_file, tokenizer, max_length)\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"加载数据集失败: {e}\")\n",
    "        print(\"请确保csv_file路径正确且文件有效。\")\n",
    "        return # 如果数据集加载失败则退出\n",
    "\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    try:\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    except ValueError as e:\n",
    "        print(f\"分割数据集时出错 (训练集大小={train_size}, 验证集大小={val_size}, 总数={len(dataset)}): {e}\")\n",
    "        print(\"确保您的数据集有足够的样本进行分割。\")\n",
    "        return\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0) # num_workers=0以简化，可根据需要调整\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    print(f\"数据集已加载: {len(dataset)} 个样本\")\n",
    "    print(f\"训练集大小: {len(train_dataset)}\")\n",
    "    print(f\"验证集大小: {len(val_dataset)}\")\n",
    "\n",
    "    # --- 设置优化器和损失函数 ---\n",
    "    # 只优化回归头的参数（fc1和regressor）\n",
    "    optimizer = AdamW(list(model.fc1.parameters()) + list(model.regressor.parameters()), lr=learning_rate)\n",
    "    criterion = nn.MSELoss() # 用于回归的均方误差损失，换MAE-torch.nn.L1Loss()?\n",
    "\n",
    "    # --- 开始训练 ---\n",
    "    best_val_loss = float('inf') # 初始化为无穷大\n",
    "\n",
    "    print(\"开始训练...\")\n",
    "    for epoch in range(epochs):\n",
    "        # 训练一个轮次\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "        # 在验证集上评估\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"轮次 {epoch+1}/{epochs}: 训练损失 = {train_loss:.4f}, 验证损失 = {val_loss:.4f}\")\n",
    "\n",
    "        # 如果验证损失改善则保存模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # 保存整个模型的状态字典\n",
    "            # 可以改成只保存回归头\n",
    "            torch.save(model.state_dict(), output_model_file)\n",
    "            print(f\"验证损失已改善。模型已保存至 {output_model_file}\")\n",
    "\n",
    "    print(\"训练完成。\")\n",
    "    print(f\"最佳验证损失: {best_val_loss:.4f}\")\n",
    "    print(f\"最佳模型已保存至: {output_model_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392385f0",
   "metadata": {},
   "source": [
    "## 推理代码，加载训练好的模型，进行增强子活性预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c129d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def predict_dna_activity(sequence, model, tokenizer, device, max_length=512):\n",
    "    \"\"\"\n",
    "    对单个DNA序列进行活性预测。\n",
    "\n",
    "    参数:\n",
    "        sequence (str): DNA序列字符串。\n",
    "        model (nn.Module): 加载了权重的模型。\n",
    "        tokenizer (object): Evo2分词器。\n",
    "        device (torch.device): 'cuda' 或 'cpu'。\n",
    "        max_length (int): 序列最大长度。\n",
    "\n",
    "    返回:\n",
    "        float: 预测的活性值。\n",
    "    \"\"\"\n",
    "    # 将模型设置为评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    # 手动对序列进行分词和填充/截断\n",
    "    token_ids = tokenizer.tokenize(sequence)\n",
    "    if len(token_ids) > max_length:\n",
    "        token_ids = token_ids[:max_length]\n",
    "    else:\n",
    "        padding_length = max_length - len(token_ids)\n",
    "        pad_token_id = getattr(tokenizer, 'pad_token_id', 1)  # charleveltokenizer 的 pad token id = 1\n",
    "        token_ids.extend([pad_token_id] * padding_length)\n",
    "    \n",
    "    input_ids = torch.tensor([token_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    # 禁用梯度计算以进行推理\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_ids)\n",
    "        \n",
    "    return prediction.item()\n",
    "\n",
    "def run_inference():\n",
    "    \"\"\"\n",
    "    主推理函数，处理用户交互和预测流程。\n",
    "    \"\"\"\n",
    "    # --- 配置参数 (应与训练时保持一致) ---\n",
    "    model_name = \"evo2_7b\"\n",
    "    output_dir = \"evo2_regression_output\"\n",
    "    model_path = os.path.join(output_dir, f\"{model_name}_regression.pt\")\n",
    "    test_csv_path = \"test_data.csv\"\n",
    "    \n",
    "    max_length = 512\n",
    "    intermediate_hidden_size = 512\n",
    "    dropout_rate = 0.1  # 在评估模式下不生效，但模型初始化需要\n",
    "\n",
    "    # --- 环境设置 ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"使用设备: {device}\")\n",
    "\n",
    "    # --- 加载模型和分词器 ---\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"错误: 未找到训练好的模型文件 {model_path}。请先运行训练代码。\")\n",
    "        return\n",
    "\n",
    "    print(\"正在加载模型...\")\n",
    "    try:\n",
    "        model = Evo2ForRegression(model_name=model_name, dropout_rate=dropout_rate, intermediate_size=intermediate_hidden_size)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()  # 确保模型处于评估模式\n",
    "        tokenizer = model.evo2_wrapper.tokenizer\n",
    "        print(\"模型加载成功。\")\n",
    "    except Exception as e:\n",
    "        print(f\"加载模型时出错: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 模式选择 ---\n",
    "    while True:\n",
    "        print(\"\\n请选择预测模式:\")\n",
    "        print(\"1. 从文件加载并预测 (需要 test_data.csv)\")\n",
    "        print(\"2. 手动输入序列预测\")\n",
    "        print(\"3. 退出\")\n",
    "        choice = input(\"请输入选项 (1/2/3): \").strip()\n",
    "\n",
    "        if choice == '1':\n",
    "            # --- 模式1: 从文件预测 ---\n",
    "            if not os.path.exists(test_csv_path):\n",
    "                print(f\"错误: 测试文件 {test_csv_path} 不存在。\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"从文件 {test_csv_path} 加载数据并进行预测...\")\n",
    "            try:\n",
    "                df = pd.read_csv(test_csv_path)\n",
    "                if 'sequence' not in df.columns or 'expression' not in df.columns:\n",
    "                    print(\"错误: CSV文件必须包含 'sequence' 和 'expression' 列。\")\n",
    "                    continue\n",
    "\n",
    "                predictions = []\n",
    "                for seq in tqdm(df['sequence'], desc=\"预测中\"):\n",
    "                    pred_val = predict_dna_activity(str(seq), model, tokenizer, device, max_length)\n",
    "                    predictions.append(pred_val)\n",
    "                \n",
    "                df['expression_predict'] = predictions\n",
    "                df['deviation'] = df['expression'] - df['expression_predict']\n",
    "\n",
    "                print(\"\\n--- 预测结果 ---\")\n",
    "                for _, row in df.iterrows():\n",
    "                    seq_display = (row['sequence'][:40] + '...') if len(row['sequence']) > 40 else row['sequence']\n",
    "                    print(f\"序列: {seq_display:<45} | 真实值: {row['expression']:<10.4f} | 预测值: {row['expression_predict']:<10.4f} | 偏差: {row['deviation']:<10.4f}\")\n",
    "                \n",
    "                mse = mean_squared_error(df['expression'], df['expression_predict'])\n",
    "                print(\"--------------------------------------------------\")\n",
    "                print(f\"\\n所有活性值预测的均方误差 (MSE): {mse:.4f}\")\n",
    "                break  # 完成文件预测后退出程序\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"处理文件时出错: {e}\")\n",
    "            \n",
    "        elif choice == '2':\n",
    "            # --- 模式2: 手动输入预测 ---\n",
    "            print(\"\\n已进入手动输入模式。输入 'exit' 或 'quit' 结束。\")\n",
    "            while True:\n",
    "                sequence = input(\"请输入DNA序列: \").strip().upper()\n",
    "                if sequence.lower() in ['exit', 'quit']:\n",
    "                    break\n",
    "                if not sequence:\n",
    "                    continue\n",
    "                \n",
    "                if any(c not in 'ATCGN' for c in sequence):\n",
    "                    print(\"错误: 序列包含无效字符。请只使用 A, T, C, G, N。\")\n",
    "                    continue\n",
    "\n",
    "                prediction = predict_dna_activity(sequence, model, tokenizer, device, max_length)\n",
    "                print(f\"  => 预测的活性值: {prediction:.4f}\\n\")\n",
    "        \n",
    "        elif choice == '3':\n",
    "            print(\"程序已退出。\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"无效输入，请输入 1, 2, 或 3。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "# 运行推理主函数\n",
    "run_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
